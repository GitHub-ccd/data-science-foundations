{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "As we saw regularization was one of the tools that we had to fight the bias variance tradeoff. But we did not see how to effectively use it. We saw that it was able to, with fine grain precision, determine the size of our hypothesis set. We of course know that the size of the hypothesis set is crucial to the final out of sample performance. \n",
    "\n",
    "But how do we determine what is the best size for the hypothesis set?\n",
    "\n",
    "We can determine this using by iterating over a set of regularization parameters and then determining which had the best results. An important note here is that we will be evaluating the final performance on a validation set and this will be able to tell us which hypothesis will do best on outside data. \n",
    "\n",
    "Let's go ahead and try this with our decision tree to see if we can get better out of sample performance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# we now consolidate the preprocessing\n",
    "def billionaire_preprocess():\n",
    "    data = pd.read_csv('../data/billionaires.csv')\n",
    "\n",
    "    del data['was founder']\n",
    "    del data['inherited']\n",
    "    del data['from emerging']\n",
    "\n",
    "    data.age.replace(-1, np.NaN, inplace=True)\n",
    "    data.founded.replace(0, np.NaN, inplace=True)\n",
    "    data.gdp.replace(0, np.NaN, inplace=True)\n",
    "    \n",
    "    del data['company.name']\n",
    "    del data['name']\n",
    "    del data['country code']\n",
    "    del data['citizenship']\n",
    "    del data['rank']\n",
    "    del data['relationship']\n",
    "    del data['sector']\n",
    "    \n",
    "    dummy_data = pd.get_dummies(data, dummy_na=True, columns=data.select_dtypes(exclude=['float64']), drop_first=True)\n",
    "    \n",
    "    return dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1672, 70) (419, 70) (523, 70)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# now we get the data\n",
    "data = billionaire_preprocess()\n",
    "\n",
    "# we parse out the target\n",
    "y = data['worth in billions']\n",
    "del data['worth in billions']\n",
    "\n",
    "# we make our test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# and we make our validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "print X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# We then consolidate the data munging code\n",
    "def billionaire_feature_eng(X, y, quantitative_pipeline, aggregated_pipeline, training=False):\n",
    "    data = X.copy()\n",
    "\n",
    "    qualitative_features = data.select_dtypes(exclude=['float64'])\n",
    "    quantitative_features = data.select_dtypes(include=['float64'])\n",
    "    \n",
    "    # notice how we only fit on the training data!\n",
    "    if training:\n",
    "        quant_X = quantitative_pipeline.fit_transform(quantitative_features)\n",
    "    else:\n",
    "        quant_X = quantitative_pipeline.transform(quantitative_features)\n",
    "\n",
    "    X = np.concatenate([quant_X, qualitative_features], axis=1)\n",
    "    \n",
    "    if training:\n",
    "        X = aggregated_pipeline.fit_transform(X, y)\n",
    "    else:\n",
    "        X = aggregated_pipeline.transform(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# and we can abstract out specific parts of the pipeline\n",
    "quantitative_pipeline = Pipeline([\n",
    "    ('imputer', Imputer(strategy='median'))\n",
    "])\n",
    "\n",
    "aggregated_pipeline = Pipeline([\n",
    "    ('var_threshold', VarianceThreshold(threshold=0.0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = billionaire_feature_eng(X_train, y_train, quantitative_pipeline, aggregated_pipeline, training=True)\n",
    "\n",
    "X_val, y_val = billionaire_feature_eng(X_val, y_val, quantitative_pipeline, aggregated_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "reg = DecisionTreeRegressor(max_depth=5, min_samples_leaf=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that I show two ways to regularize my decision tree (there are more). And notice that these fit quite nicely into the paradigm of reducing the hypothesis set. Instead we would be looking at trees with a max depth of five or looking where their leaves can only be so small.\n",
    "\n",
    "This is perhaps even easier to understand than weight penalization (the most common form of regularization), where you would add to the loss function the sum of the absolute values or squares of the weights.\n",
    "\n",
    "So what we would do is iterate over different regularization strengths and choose the one that did best on the validation set. Let's show how you would do this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max depth 2\n",
      "min samples leaf 5\n",
      "error 2.23172876882\n",
      "max depth 2\n",
      "min samples leaf 10\n",
      "error 2.18984332729\n",
      "max depth 2\n",
      "min samples leaf 20\n",
      "error 2.1827550218\n",
      "max depth 4\n",
      "min samples leaf 5\n",
      "error 2.22345650613\n",
      "max depth 4\n",
      "min samples leaf 10\n",
      "error 2.24114147033\n",
      "max depth 4\n",
      "min samples leaf 20\n",
      "error 2.20703925261\n",
      "max depth 6\n",
      "min samples leaf 5\n",
      "error 2.24137734624\n",
      "max depth 6\n",
      "min samples leaf 10\n",
      "error 2.26024532185\n",
      "max depth 6\n",
      "min samples leaf 20\n",
      "error 2.22405848899\n",
      "max depth 8\n",
      "min samples leaf 5\n",
      "error 2.22950176744\n",
      "max depth 8\n",
      "min samples leaf 10\n",
      "error 2.210377466\n",
      "max depth 8\n",
      "min samples leaf 20\n",
      "error 2.29416057636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# here is where we do grid search\n",
    "for max_depth in range(2, 10, 2):\n",
    "    for min_samples_leaf in [5, 10, 20]:\n",
    "        reg = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "        reg.fit(X_train, y_train)\n",
    "        \n",
    "        error = mean_absolute_error(reg.predict(X_val), y_val)\n",
    "        \n",
    "        print('max depth {}'.format(max_depth))\n",
    "        print('min samples leaf {}'.format(min_samples_leaf))\n",
    "        print('error {}'.format(error))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we achieve the best out of sample error somewhere in the middle, where the regularization is not highest, but is high enough.\n",
    "\n",
    "Of course we should be concerned if we are doing this too much. We don't want to run too too many experiments. If we run too many the errors that we get on the validation set may not be reflective or reality. But the above has shown us that we can get great performance just by changing the hyperparameter (those not learned) of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

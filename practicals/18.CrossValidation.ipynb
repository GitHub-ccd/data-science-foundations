{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The full picture\n",
    "\n",
    "With cross validation we can show you the full picture of model building (after you have done the hard work of data munging). The magic that cross validation unlocks is twofold\n",
    "\n",
    "1. It allow you to have more training data and therefore get better performance and more accurate representations of your performance\n",
    "2. It actually simplifies the process. You will no longer need to keep 3 sets of data and you can get by with just two in your mental model\n",
    "\n",
    "Let's get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# we still need to do the preprocessing\n",
    "def billionaire_preprocess():\n",
    "    data = pd.read_csv('../data/billionaires.csv')\n",
    "\n",
    "    del data['was founder']\n",
    "    del data['inherited']\n",
    "    del data['from emerging']\n",
    "\n",
    "    data.age.replace(-1, np.NaN, inplace=True)\n",
    "    data.founded.replace(0, np.NaN, inplace=True)\n",
    "    data.gdp.replace(0, np.NaN, inplace=True)\n",
    "    \n",
    "    del data['company.name']\n",
    "    del data['name']\n",
    "    del data['country code']\n",
    "    del data['citizenship']\n",
    "    del data['rank']\n",
    "    del data['relationship']\n",
    "    del data['sector']\n",
    "    \n",
    "    dummy_data = pd.get_dummies(data, dummy_na=True, columns=data.select_dtypes(exclude=['float64']), drop_first=True)\n",
    "    \n",
    "    return dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2091, 70) (523, 70)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# now we get the data\n",
    "data = billionaire_preprocess()\n",
    "\n",
    "# we parse out the target\n",
    "y = data['worth in billions']\n",
    "del data['worth in billions']\n",
    "\n",
    "# we make our test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# and we no longer make a validation set!\n",
    "\n",
    "print X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# We still do feature engineering\n",
    "def billionaire_feature_eng(X, y, quantitative_pipeline, aggregated_pipeline, training=False):\n",
    "    data = X.copy()\n",
    "\n",
    "    qualitative_features = data.select_dtypes(exclude=['float64'])\n",
    "    quantitative_features = data.select_dtypes(include=['float64'])\n",
    "    \n",
    "    # notice how we only fit on the training data!\n",
    "    if training:\n",
    "        quant_X = quantitative_pipeline.fit_transform(quantitative_features)\n",
    "    else:\n",
    "        quant_X = quantitative_pipeline.transform(quantitative_features)\n",
    "\n",
    "    X = np.concatenate([quant_X, qualitative_features], axis=1)\n",
    "    \n",
    "    if training:\n",
    "        X = aggregated_pipeline.fit_transform(X, y)\n",
    "    else:\n",
    "        X = aggregated_pipeline.transform(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# and we can abstract out specific parts of the pipeline\n",
    "quantitative_pipeline = Pipeline([\n",
    "    ('imputer', Imputer(strategy='median'))\n",
    "])\n",
    "\n",
    "aggregated_pipeline = Pipeline([\n",
    "    ('var_threshold', VarianceThreshold(threshold=0.0))\n",
    "])\n",
    "\n",
    "X_train, y_train = billionaire_feature_eng(X_train, y_train, quantitative_pipeline, aggregated_pipeline, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step will be to define the model that we are looking at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "reg = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we determine which parameters we would like to search over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': range(2, 20, 2),\n",
    "    'min_samples_leaf': range(5, 25, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we use GridSearchCV which will search over the parameters doing cross validation to determine their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(reg, params, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [2, 4, 6, 8, 10, 12, 14, 16, 18], 'min_samples_leaf': [5, 10, 15, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a lot of goodies. We can see the best score and estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.2413675289858621"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=10, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get to use the grid search object as that estimator as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.86509622,  2.86509622,  2.86509622,  9.35714286,  4.31932021])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.predict(X_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last treat, we can combine all of these steps into a single pipeline and then do grid search over a variety of parameters on the different steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2091, 70) (523, 70)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# now we get the data\n",
    "data = billionaire_preprocess()\n",
    "\n",
    "# we parse out the target\n",
    "y = data['worth in billions']\n",
    "del data['worth in billions']\n",
    "\n",
    "# we make our test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# and we no longer make a validation set!\n",
    "\n",
    "print X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first we define the full pipeline\n",
    "full_pipeline = Pipeline([\n",
    "    ('imputer', Imputer(strategy='median')),\n",
    "    ('var_threshold', VarianceThreshold(threshold=0.0)),\n",
    "    ('tree', DecisionTreeRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then define the parameters that we want to search over\n",
    "params = {\n",
    "    'imputer__strategy': ['median', 'mean'],\n",
    "    'var_threshold__threshold': [0.0, 0.1, 0.2],\n",
    "    'tree__max_depth': range(2, 20, 2),\n",
    "    'tree__min_samples_leaf': range(5, 25, 5)\n",
    "}\n",
    "# notice the double underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('var_threshold', VarianceThreshold(threshold=0.0)), ('tree', DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best'))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'imputer__strategy': ['median', 'mean'], 'tree__max_depth': [2, 4, 6, 8, 10, 12, 14, 16, 18], 'var_threshold__threshold': [0.0, 0.1, 0.2], 'tree__min_samples_leaf': [5, 10, 15, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(full_pipeline, params, scoring='neg_mean_absolute_error')\n",
    "\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.24136752899\n",
      "Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('var_threshold', VarianceThreshold(threshold=0.0)), ('tree', DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=10, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "           splitter='best'))])\n"
     ]
    }
   ],
   "source": [
    "print gs.best_score_\n",
    "print gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
